# Gleason_grading

## This repository contains the relevant code to utilize a Convolutional Neural Network to classify the Gleason grades of prostate cancer. 

The image database is sourced from the Automated Gleason Grading Challenge 2022 (AGGC2022). 

## To run the model, Please see `demo.py` and its output directory `demo_result`
In order to run the model, we need to first process the image and feed the preprocessed img into our trained model. 
The `demo.py` contrained the following 2 steps:
### 1. Preprocessing Step: 
Inputs: Original slide image, annotation masks, output_patch_folder
Outputs: Patch masks for the complete image and all 5 classes 

`preprocess_img.py` contains all preprocessing steps including 
* Image segmentation into 256 x 256 patches using the Patchify library
    
Additionally, for images that OpenCV is not suited to load in, there are two other versions: `preprocess_img_openslide.py` uses the Openslide library to load in images, and `preprocess_img_pyvips.py` uses the Pyvips library.
 
The directory names and appropriate library file locations must be updated within the `demo.py`.

### 2. Run saved model:
Inputs: Original slide image, pretrained_model_parameter,out_directory_path ,output_patch_folder
Outputs: 
1. G.npy  & Ground_truth.jpg :size of height x weight  (the pixel value is the class label) (0:empty background, 1:normal, etc)
2. G_pred.jpg  & G_pred.npy   :size of height x weight (where the pixel value is the predicted class label) (0:empty background, 3. 1:normal, etc)
3. H.npy: heatmap H with the size of height x weight x 5 (where the pixel value is the Softmax of predicted Logits)
4. result.pck  : saved y-probability, true label, and index in pickel file, which is a dictionary.
    output[“yprob”] = yprob (size: n*5)
    output[“ytrue”] = label (size: n)
    output[“index_x”] = index_list_x (size: n)
    output[“index_y”] = index_list_y (size: n)

## Neural Network Training:
### Model structure:
The first part is a feature encoder using DenseNet-121 (saved under `mtdp`) as the backbone, and the model parameters pre-trained by the histopathology images are used as model initialization. In the second part, we utilized a multi-layer perception (saved under `Model`), which consists of two linear transformation layers and a rectified linear unit (ReLU) non-linear activation layer, to reduce the dimensionality of feature map to the 5 desired classifications. 

### Training process:
To reduce the time-cost of the training process, we extracted the features from the last convolutional block before the fully connected layer (FCN), which outputs a 1024-dimensional vector for each patch and only trained the multi-layer perception. Through multiple hyperparameter tuning, the best model performance is generated by a stochastic gradient descent (SGD) optimizer with a learning rate of 0.0001 updating the model parameters for each training cycle, or epoch, after which the Balanced Cross Entropy Loss was calculated. The loss and accuracy were continuously monitored in the training process for 100 epochs. To prevent overfitting problem, we utilized augmentation methods including vertical flipping, color transformation, and affine transformation. The augmentation methods were only applied on the training data, and the normalization method was applied to all datasets. Besides, we added a dropout layer after the rectified linear unit (ReLU) non-linear activation layer. The training and validation dataset was spilt at the WSI level with the ratio of 8:2. The final model was selected by the best accuracy for the validation dataset. 

* Trained model parameters are saved under `checkpoint folder`

